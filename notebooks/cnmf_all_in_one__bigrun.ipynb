{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cNMF in `cellarium-ml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stephen Fleming, Yang Xu\n",
    "\n",
    "2025.07.31\n",
    "\n",
    "The `cellarium-ml` project:\n",
    "\n",
    "https://github.com/cellarium-ai/cellarium-ml\n",
    "\n",
    "The specific implementation of cNMF we are actively working on:\n",
    "\n",
    "https://github.com/cellarium-ai/cellarium-ml/pull/196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "`cellarium-ml` implements a variety of algorithms in a way that is scalable to hundreds of millions of cells and beyond.\n",
    "This notebook provides a demo run of Cellarium's implementation of consensus NMF (cNMF).\n",
    "\n",
    "Here we demonstrate our ability to run on a large dataset of 4.17M cells.\n",
    "\n",
    "Because this is intended to run on a laptop, cellarium is running in \"streaming\" mode where it is continuously downloading files from a google bucket.\n",
    "This is not the fastest way to run: downloading all curriculum h5ads locally first is much much faster.\n",
    "This notebook is just a demonstration that it is possible to run without doing a full download all at once.\n",
    "\n",
    "The specific algorithm for NMF is based on \"Online learning for matrix factorization and sparse coding\" by Mairal, Bach, Ponce, and Sapiro (JMLR 2010).\n",
    "\n",
    "## This notebook\n",
    "\n",
    "This notebook shows an end-to-end cNMF run in `cellarium-ml`, starting with h5ad files and ending with results.\n",
    "There are several steps involved.\n",
    "\n",
    "## Description of analysis steps\n",
    "\n",
    "1. Compute highly-variable genes.\n",
    "\n",
    "2. Run cNMF on selected highly-variable genes.\n",
    "\n",
    "3. Interactive plotting in this notebook to help determine optimal number of programs `k`, and a \n",
    "   `density_threshold` and `local_neighborhood_size` for the consensus step.\n",
    "\n",
    "4. Computing consensus factors.\n",
    "\n",
    "5. (Optional) Computing per-cell factor loadings.\n",
    "\n",
    "6. (Optional) Re-computing the `k` factor definitions using all genes (not just highly-variable genes).\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- a lits of filepaths to h5ad files: can be local or in a google bucket (or at some URL)\n",
    "    - ideally the h5ad files would be from an extracted curriculum from `cellarium-nexus`, but these files can be any h5ad files\n",
    "        - until [#324](https://github.com/cellarium-ai/cellarium-ml/issues/324) is resolved, the h5ad files\n",
    "          should be limited in size to what can fit in memory\n",
    "\n",
    "## Outputs (work in progress... not complete)\n",
    "\n",
    "- anndata object for all cells (with an empty count matrix) containing:\n",
    "    - `adata.obsm[\"X_cnmf_k20\"]`: (cell, k) matrix of per-cell factor loadings (for the `k = 20` decomposition)\n",
    "    - `adata.obsp[\"cnmf_k20_factors_hvg\"]`: (gene, k) matrix of definitions of each of the `k` consensus programs\n",
    "        - from the initial cNMF fit: all non-highly-variable genes have weight zero\n",
    "    - `adata.obsp[\"cnmf_k20_factors_hvg_tpm\"]`: (gene, k) matrix of definitions of each of the `k` consensus programs\n",
    "        - same as above but weights are recomputed to represent TPM values via a refitting step\n",
    "    - `adata.obsp[\"cnmf_k20_factors\"]`: (gene, k) matrix of definitions of each of the `k` consensus programs\n",
    "        - computed by refitting cell loadings from `adata.obsp[\"cnmf_k20_factors_hvg\"]` by refitting the dataset including all genes\n",
    "    - `adata.obsp[\"cnmf_k20_factors_tpm\"]`: (gene, k) matrix of definitions of each of the `k` consensus programs\n",
    "        - same as above but weights are recomputed to represent TPM values via a refitting step\n",
    "    - (optionally): all of the above for other choices of `k` as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: You will need to use the `nmf_sf_singlenotebook` branch of `cellarium-ml` on github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "\n",
    "import cellarium.ml.api\n",
    "import cellarium.ml.data\n",
    "import cellarium.ml.models\n",
    "import cellarium.ml.preprocessing\n",
    "import cellarium.ml.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for demonstration purposes: automatically grab h5ad file paths from a bucket prefix, like data from Nexus\n",
    "\n",
    "example_cellarium_curriculum_h5ad_paths = cellarium.ml.api.h5ad_paths_from_google_bucket(\n",
    "    \"gs://cellarium-nexus-file-system-335649/pipeline/data-extracts/all_cells_cap_freeze1_20250721/extract_files\"\n",
    ")\n",
    "print(f\"[{example_cellarium_curriculum_h5ad_paths[0]}, ...]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually here I have mounted the bucket via gcsfuse using this command\n",
    "# ~/go/bin/gcsfuse -o ro --only-dir pipeline/data-extracts/all_cells_cap_freeze1_20250721/extract_files\n",
    "# cellarium-nexus-file-system-335649 /Users/sfleming/Desktop/fuse\n",
    "# so the files look like they are local at /Users/sfleming/Desktop/fuse\n",
    "\n",
    "example_cellarium_curriculum_h5ad_paths = [\n",
    "    f.replace(\n",
    "        \"gs://cellarium-nexus-file-system-335649/pipeline/data-extracts/all_cells_cap_freeze1_20250721/extract_files\",\n",
    "        \"/Users/sfleming/Desktop/fuse\",\n",
    "    )\n",
    "    for f in example_cellarium_curriculum_h5ad_paths\n",
    "]\n",
    "print(f\"[{example_cellarium_curriculum_h5ad_paths[0]}, ...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cellarium data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo we are using the python API for cellarium.  It's also possible to use command line versions of these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5ad_paths = example_cellarium_curriculum_h5ad_paths\n",
    "h5ad_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(h5ad_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For remote files over the internet, this next cell can take a minute.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts cells in each file, takes 10 mins\n",
    "\n",
    "limits = cellarium.ml.api.get_h5ad_files_limits(h5ad_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = cellarium.ml.CellariumAnnDataDataModule(\n",
    "    dadc=cellarium.ml.data.DistributedAnnDataCollection(\n",
    "        filenames=h5ad_paths,\n",
    "        limits=limits,\n",
    "        obs_columns_to_validate=[],\n",
    "        max_cache_size=3,\n",
    "        cache_size_strictly_enforced=True,\n",
    "    ),\n",
    "    batch_keys={\n",
    "        \"x_ng\": cellarium.ml.utilities.data.AnnDataField(attr=\"X\", convert_fn=cellarium.ml.utilities.data.densify),\n",
    "        \"var_names_g\": cellarium.ml.utilities.data.AnnDataField(attr=\"var_names\"),\n",
    "        \"obs_names_n\": cellarium.ml.utilities.data.AnnDataField(attr=\"obs_names\"),\n",
    "    },\n",
    "    batch_size=5000,\n",
    "    shuffle=True,\n",
    "    train_size=1.0,\n",
    "    prefetch_factor=2,\n",
    "    num_workers=8,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "datamodule.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: It is highly recommended in practice to run this notebook on a machine where you can actually download all the h5ad files. Everything will run much faster.\n",
    "\n",
    "Try\n",
    "```\n",
    "mkdir -p data_extract_h5ads\n",
    "gsutil -m cp gs://cellarium-nexus-file-system-335649/pipeline/data-extracts/all_cells_cap_freeze1_20250721/extract_files/*.h5ad data_extract_h5ads/\n",
    "```\n",
    "\n",
    "and then replace `h5ad_paths` above with local paths.\n",
    "\n",
    "\n",
    "Example:\n",
    "- the onepass model below takes about 1.5 hr on 4M cells over my home wifi\n",
    "- the onepass model below takes about 30 min on 4M cells over my home wifi with gcsfuse\n",
    "- a onepass model on 4M cells on a local disk should take about 10 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highly variable genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run onepass model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computes mean and variance per gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gene names to use later (and assume all files have the same genes)\n",
    "\n",
    "var_names_g = cellarium.ml.api.get_h5ad_file_var_names_g(h5ad_paths[0])\n",
    "var_names_g[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the model that will be used to compute mean and var of each gene\n",
    "\n",
    "onepass_module = cellarium.ml.CellariumModule(\n",
    "    transforms=[\n",
    "        cellarium.ml.transforms.NormalizeTotal(),\n",
    "        cellarium.ml.transforms.Log1p(),\n",
    "    ],\n",
    "    model=cellarium.ml.models.OnePassMeanVarStd(\n",
    "        var_names_g=cellarium.ml.api.get_h5ad_file_var_names_g(h5ad_paths[0]),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    "    max_epochs=1,\n",
    "    default_root_dir=\"tmp/onepass\",\n",
    ")\n",
    "trainer.fit(onepass_module, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the onepass model computes a mean and variance per gene\n",
    "\n",
    "mean_g = trainer.model.model.mean_g\n",
    "var_g = trainer.model.model.var_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute hvgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose `n_top_genes` to suit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = cellarium.ml.preprocessing.get_highly_variable_genes(\n",
    "    gene_names=var_names_g,\n",
    "    mean=mean_g,\n",
    "    var=var_g,\n",
    "    n_top_genes=2000,\n",
    ")\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var[\"highly_variable\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the highly variable genes\n",
    "\n",
    "hvg_var_names_g = var.index[var[\"highly_variable\"]]\n",
    "hvg_var_names_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cNMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set things up to run cNMF in cellarium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With data in a google bucket, 4M cells takes 1.5 hrs per epoch on my `mps` laptop with 20 `k` values and 50 NMF replicates.\n",
    "\n",
    "Using gcsfuse, it's about the same. First epoch 2 hrs, later epochs 1 hr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user's choice for the number of components: must input a python list\n",
    "\n",
    "# k_values = [10, 20, 30]\n",
    "k_values = list(range(5, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user's choice for the number of NMF replicates that should go into consensus\n",
    "\n",
    "nmf_replicates = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get set up for training\n",
    "\n",
    "nmf_model = cellarium.ml.models.NonNegativeMatrixFactorization(\n",
    "    var_names_hvg=hvg_var_names_g,\n",
    "    k_values=k_values,\n",
    "    r=nmf_replicates,\n",
    ")\n",
    "\n",
    "nmf_module = cellarium.ml.CellariumModule(\n",
    "    cpu_transforms=[cellarium.ml.transforms.Filter(filter_list=hvg_var_names_g)],\n",
    "    model=nmf_model,\n",
    ")\n",
    "\n",
    "datamodule.setup(stage=\"fit\")\n",
    "\n",
    "trainer_nmf = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=3,  # this is up for debate, but empirically 5 was enough for the donor regression benchmark\n",
    "    default_root_dir=\"tmp/nmf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on the data\n",
    "\n",
    "pl.seed_everything(0)  # not required but helps make this notebook reproducible\n",
    "\n",
    "trainer_nmf.fit(nmf_module, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the shape of the NMF gene programs that have been inferred: [replicates, k, genes]\n",
    "\n",
    "for k in nmf_model.k_values:\n",
    "    print(getattr(nmf_model, f\"D_{k}_rkg\").shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up to explore outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a helper class that facilitates downstream analysis steps. Here we instantiate it and use it to get various outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellarium.ml.models.nmf import NMFOutput\n",
    "\n",
    "nmf_output = NMFOutput(\n",
    "    nmf_module=nmf_module,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default k-selection plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what Kotliar cNMF would produce with default values for `local_neighborhood_size=0.3` and `density_threshold=0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_output.default_k_selection_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other versions of the k-selection plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kotliar does not demo this, but the k-selection plot itself depends on the values of `density_threshold` and `local_neighborhood_size`.\n",
    "\n",
    "In principle, you could choose different hyperparameters for each `k`, run `nmf_output.compute_consensus_factors(k, <your selected params here>)` on all the `k`, and then re-run `nmf_output.calculate_reconstruction_error()` and re-create the k-selection plot using `nmf_output.k_selection_plot()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know, one guiding principle for this process could be the following... for each `k`, automatically choose a (reasonable) `density_threshold` that maximizes the stability for the given `k`. Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_output.maximal_stability_k_selection_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results of this kind of automatic choosing of density_threshold\n",
    "# note that the call to plot_clustermap() recomputes consensus if density_threshold is not None\n",
    "\n",
    "for k in [10, 13, 21]:  # nmf_output.consensus:\n",
    "    nmf_output.plot_clustermap(k=k, density_threshold=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The strange thing about the stability metric is that it does not guarantee that there are actually `k` clusters.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just look at some of the genes involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_output.nmf_module.model.var_names_hvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# gene_name_lookup = adata.var['feature_name'].to_dict()\n",
    "\n",
    "factor_df = pd.DataFrame(\n",
    "    nmf_output.consensus[best_k][\"consensus_D_kg\"].t().numpy(),\n",
    "    index=nmf_output.nmf_module.model.var_names_hvg,\n",
    ")\n",
    "# factor_df['gene_name'] = factor_df.index.map(gene_name_lookup)\n",
    "factor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_df.sort_values(by=1, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_df.sort_values(by=2, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_df.sort_values(by=5, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_df.sort_values(by=6, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute per-cell loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick ten h5ad files and load data (cells are randomly shuffled)\n",
    "# in theory this could be done for all cells, but I have no use for that here\n",
    "\n",
    "n_cells_visualization = 100_000\n",
    "n_anndata_shards_visualization = n_cells_visualization // 10_000\n",
    "\n",
    "datamodule_small = cellarium.ml.CellariumAnnDataDataModule(\n",
    "    dadc=cellarium.ml.data.DistributedAnnDataCollection(\n",
    "        filenames=h5ad_paths[:n_anndata_shards_visualization],\n",
    "        limits=limits[:n_anndata_shards_visualization],\n",
    "        obs_columns_to_validate=[],\n",
    "        max_cache_size=n_anndata_shards_visualization,\n",
    "        cache_size_strictly_enforced=True,\n",
    "    ),\n",
    "    batch_keys={\n",
    "        \"x_ng\": cellarium.ml.utilities.data.AnnDataField(attr=\"X\", convert_fn=cellarium.ml.utilities.data.densify),\n",
    "        \"var_names_g\": cellarium.ml.utilities.data.AnnDataField(attr=\"var_names\"),\n",
    "        \"obs_names_n\": cellarium.ml.utilities.data.AnnDataField(attr=\"obs_names\"),\n",
    "    },\n",
    "    batch_size=5000,\n",
    "    shuffle=False,\n",
    "    train_size=1.0,\n",
    "    prefetch_factor=None,\n",
    "    num_workers=None,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "datamodule_small.setup(stage=\"predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loadings of each factor, computed for each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get per-cell factor loadings using the best k: this takes time\n",
    "# `normalize` controls whether the per-cell loadings sum to 1\n",
    "\n",
    "df = nmf_output.compute_loadings(k=best_k, datamodule=datamodule_small, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory could add this information to the anndata object if you had a single object.\n",
    "Here we will assume the dataset might be very large in total, so we will just try to grab a chunk of data and add the annotations for those cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grab cells as anndata object (this might take time to download data from bucket)\n",
    "\n",
    "adata = datamodule_small.dadc[:n_cells_visualization]\n",
    "if adata.raw is not None:\n",
    "    adata.layers[\"counts\"] = adata.raw.X.copy()\n",
    "else:\n",
    "    adata.layers[\"counts\"] = adata.X.copy()\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cNMF loadings to obsm\n",
    "\n",
    "adata.obsm[\"X_nmf\"] = df.loc[adata.obs_names].values\n",
    "adata.obsm[\"X_nmf\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize factor loadings on a UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, if you have scanpy installed in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.set_figure_params(figsize=(5, 5), fontsize=14, vector_friendly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(adata, layer=\"counts\", flavor=\"seurat_v3\", n_top_genes=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X = adata.layers[\"counts\"].copy()\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)\n",
    "# sc.pp.scale(adata, max_value=10)\n",
    "sc.tl.pca(adata, use_highly_variable=True)\n",
    "sc.pp.neighbors(adata, method=\"umap\", metric=\"cosine\", n_pcs=10)\n",
    "sc.tl.umap(adata)\n",
    "adata.obsm[\"X_umap_counts\"] = adata.obsm[\"X_umap\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put these in obs for plotting\n",
    "\n",
    "for k in range(adata.obsm[\"X_nmf\"].shape[1]):\n",
    "    adata.obs[f\"nmf_{k}\"] = adata.obsm[\"X_nmf\"][:, k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=\"umap_counts\",\n",
    "    color=[\"cell_type\", \"brain_region_abbreviation\", \"cohort\", \"scpred_class\", \"village\"],\n",
    "    # color_map='Oranges',\n",
    "    ncols=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(adata, basis=\"umap_counts\", color=[f\"nmf_{i}\" for i in range(0, best_k)], color_map=\"Oranges\", ncols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a UMAP derived from the NMF components\n",
    "\n",
    "sc.pp.neighbors(adata, use_rep=\"X_nmf\", method=\"umap\", metric=\"cosine\")\n",
    "sc.tl.umap(adata)\n",
    "adata.obsm[\"X_umap_nmf\"] = adata.obsm[\"X_umap\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(adata, basis=\"umap_nmf\", color=[\"scpred_class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely a bit wonky, but also definitely picking up on cell types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(adata, basis=\"umap\", color=[\"neuropath_diagnosis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project factors back to all genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now refit for all genes, not just the highly variable genes. In cNMF this involves solving an auxiliary linear regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results as a summary anndata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results can be packaged up into an anndata object if desired, and perhaps saved that way as an h5ad file.\n",
    "\n",
    "Here we omit the actual count matrix, since in theory it is too big to fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "cellarium",
   "name": "common-cu122.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu122:m121"
  },
  "kernelspec": {
   "display_name": "cellarium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
